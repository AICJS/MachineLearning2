synthetic.csv 데이터를 활용해서 회귀분석을 진행하고, VIF 값이 7 초과인 변수와 이상치를 제거한 후 
OLS와 GD의 각각의 R-Squared 값과 회귀계수 값을 도출하시오 (Learning rate 는 0.005)



install.packages("MASS") # Boston 데이터셋, 변수선택법
install.packages("car") # vif, durbinWatsonTest
install.packages("skimr") # 데이터 요약
install.packages("rstatix") # shapiro_test
install.packages("caret") # 데이터 분할
install.packages("BB")

library(MASS)
library(car)
library(skimr)
library(rstatix)
library(caret)
library(BB)

C:/Users/PC502/Desktop



df <- read.csv("C:/Users/PC502/Desktop/synthetic.csv")
skim(df)
initial_predictors <- c("x3", "x4", "x5", "x6", "x7", "x8", "x9", "x10")
train_indices <- createDataPartition(df$y, p = 0.8, list = FALSE)
result <- lm(df$y ~ ., data = df[,initial_predictors])
summary(result)

source("C:/Users/PC502/Desktop/Gradient Descent.R")
train_indices <- createDataPartition(df$y, p = 0.8, list = FALSE)


# 학습
gd_model <- train_bb_lm(df, target = "y", predictors = initial_predictors, 12.1e-4)

# 결과요약
train_metrics.bb_lm(gd_model, df)

# 저장
saveRDS(gd_model, file = "bb_lm_synthetic.rds")

# 불러오기
loaded <- readRDS("bb_lm_synthetic.rds")



# 예측 & 평가
pred <- predict(loaded, newdata = df)
rmse <- sqrt(mean((df$y - pred)^2))
r2 <- 1 - sum((df$y - pred)^2) / sum((df$y - mean(df$y))^2)

print(round(pred, 3))
print(paste("RMSE:", round(rmse, 3)))
print(paste("R-Squared:", round(r2, 3)))

# 원본 스케일의 최종 계수 확인
coef(loaded)



# 잔차
opar <- par(no.readonly = TRUE) # 그래프가 들어갈 window 생성
par(mfrow = c(2, 2)) # 그래프가 들어갈 공간 생성 2X2
plot(result) # 그래프 결과 도출
par(opar) # 그래픽 매개변수 복원

# 정규성 : p > 0.05 보다 크면 귀무가설 채택(정규성이 있음)
shapiro_test(result$residuals)



# 다중공선성(변수의 중복성) : 10 이상 GVIF^(1/(2*DF))
vif_values <- vif(result)

print(vif_values)

selected_predictors <- names(vif_values[vif_values < 7])
print(selected_predictors)

# 이상치검정 : cook's D 값이 높으면 문제(y 값이 3 ~ 4 보다 크면)
influencePlot(result, id.method = "identify")

# 이상치 제거
df = df[-c(42, 301, 330, 551, 1008, 1011),] # 여러 개의 값 제거



df <- read.csv("C:/Users/PC502/Desktop/diabetes.csv", header = TRUE, na = ".")
result <- lm(df$Diabetes ~ ., data = df)


# AIC(Akaike Information Criterion) AIC 값이 줄어들면 그 변수는 의미가 있음

# 후진소거법
result_bk = lm(df$Diabetes ~ ., data = df)
Df_fit_bk = stepAIC(result_bk, direction = "backward", trace = T) # trace 하나씩 빼는 과정을 보여줌

# 단계적선택법
Df_fit_st = stepAIC(result_bk, direction = "both", trace = T)

# 전진선택법
result_fw = lm(df$Diabetes ~ 1, data = df)
Df_fit_fw = stepAIC(result_fw, direction = "forward", scope = (df$Diabetes ~ Pregnancies + SkinThickness + Insulin + Age + Outcome), trace = T) # scope = (Backward 변수 카피)
